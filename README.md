# Lung CT Image Generation using Conditional Normalizing Flows (CNF-UNet)

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://achintha9533.github.io/Pattern-Recognition/license.html)
[![Python 3.9+](https://img.shields.io/badge/Python-3.9%2B-blue.svg)](https://www.python.org/downloads/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0%2B-orange.svg)](https://pytorch.org/)


**Lung CT Image Generation using CNF-UNet** is a deep learning tool designed to synthesize realistic Lung CT images from Gaussian noise. It leverages a **Conditional Normalizing Flow (CNF) architecture with a U-Net backbone**, focusing on learning a continuous transformation from a simple prior distribution to complex medical image data. This project, developed for the final examination of the Software and Computing course at the University of Bologna, emphasizes **reproducible and modular deep learning workflows**.

This tool is built for:

  * **Reproducible Results:** Designed for consistent output generation given the same inputs and environment.
  * **Modular Design:** The codebase is structured into distinct, testable modules for clarity and maintainability.
  * **Comprehensive Evaluation:** Includes standard and perceptual metrics for assessing generated image quality.

It also features:

  * **Comprehensive API Documentation:** Automated generation of detailed API documentation using Sphinx from reStructuredText-formatted docstrings.
  * Fully documented source code (via docstrings and comments).
  * Image preprocessing, model training, inference, and visualization steps.
  * Automated testing with high coverage using `pytest`.
  * Centralized configuration for easy modification of parameters.

-----

## Installation & Setup

This project has been tested with **Python 3.9+** on macOS and Windows and should work with compatible PyTorch versions. Refer to `requirements.txt` for the full list of dependencies.

To get started, follow these steps:

1.  **Clone the repository:**

    ```bash
    git clone https://github.com/Achintha9533/Pattern-Recognition.git
    cd Pattern-Recognition
    ```

2.  **Create and activate a virtual environment (recommended):**
    It's highly recommended to use the virtual environment to manage project dependencies and avoid conflicts.

    On macOS/Linux:

    ```bash
    source venv/bin/activate
    ```

    On Windows:

    ```bash
    .\venv\Scripts\activate
    ```

    Your terminal prompt should show `(venv)` or `(3.9)` at the beginning, indicating the virtual environment is active.

3.  **Install dependencies:**
    With your virtual environment activated, install all required Python packages:

    ```bash
    pip install --upgrade pip
    ```
    ```bash  
    pip install -r requirements.txt
    ```
  ## Usage

  The primary way to use the application is through `main.py`.

  To execute the complete workflow, from obtaining the code to generating synthetic images and the final report, follow these steps. The entire process, from cloning to generating the report, typically takes approximately 15 minutes, depending on your system specifications and internet connection.

  On macOS/Linux:(Tested for the macOS)

  ```bash
  python synthetic_image_generator/main.py
  ```

  -----

## Example Outputs

This section provides visual examples generated by the Conditional Normalizing Flows (CNF-UNet) model, along with relevant analysis.

-----

**Example of a Synthetically Generated Lung CT Image**

This figure displays a representative synthetically generated Lung CT image, demonstrating the direct output quality of the model.

-----

**Comparison of Generated vs. Real Images**

This figure provides a visual comparison between an image generated by the model and a corresponding real Lung CT image from the dataset. It allows for a direct qualitative assessment of the synthetic data’s realism.

*Visual comparison showing a generated image next to a real image.*

-----

**Distribution of Image Characteristics**

This figure illustrates the distribution of certain image characteristics or metrics (e.g., pixel intensities, texture features) across both generated and real datasets. This helps in quantitatively assessing the similarity of the distributions.

*Comparison of feature distributions for real vs. generated images.*

-----

## Documentation

This project includes complete documentation to help users understand, use, and extend the codebase. It covers usage instructions, command-line options, example outputs, testing setup, and detailed API reference generated from inline docstrings. The structure and design choices are also explained for easier onboarding and maintainability.

For a complete guide, please see the [full documentation here](https://achintha9533.github.io/Pattern-Recognition/).


-----

## Model Architecture

This section provides a detailed overview of the Conditional Normalizing Flow with UNet (CNF-UNet) architecture employed in this project for Lung CT image generation. Understanding this architecture is key to grasping how the model learns to generate high-quality synthetic images.

### Overview of CNF-UNet

The core of this project’s image generation capability lies in the integration of Conditional Normalizing Flows (CNF) with a UNet-like structure. This combination allows for a powerful generative model that can learn complex data distributions while leveraging the hierarchical feature extraction benefits of the UNet.

[Elaborate here on the high-level concept: How CNF works (invertible transformations, exact likelihood), how UNet contributes (multi-scale feature extraction, skip connections), and how conditioning is applied (e.g., through UNet features).]

### Normalizing Flows (NF)

Normalizing Flows are a class of generative models that transform a simple base distribution (e.g., a Gaussian) into a complex data distribution through a sequence of invertible and differentiable transformations. This allows for exact likelihood computation and efficient sampling.

[Detail the specific types of flow layers used (e.g., coupling layers, permutations, non-linearities) and why they were chosen.]

### UNet Integration

The UNet architecture, originally developed for biomedical image segmentation, is well-suited for processing images due to its encoder-decoder structure with skip connections. In our CNF-UNet, the UNet part typically acts as a powerful feature extractor that provides rich, multi-scale conditional information to the normalizing flow.

[Explain how the UNet is used: Does it provide features at different scales to different flow blocks? Is it an encoder for the conditioning? How are skip connections utilized in this context?]

### Conditional Aspect

The “Conditional” aspect of CNF-UNet means the model’s generation process is guided by certain input conditions. This allows for controlled image synthesis.

[Describe what the conditioning variables are (e.g., clinical parameters, partial images, noise levels) and how they are incorporated into the CNF and/or UNet parts of the architecture. How does this conditioning influence the generated output?]

### Component Details

  * **UNetBlock:** [Describe the structure of your basic UNet building block (e.g., convolutional layers, activation functions, batch normalization). You can link to its API documentation here.]
  * **Flow Layers:** [Detail the specific flow layers like Affine Coupling Layers, Invertible 1x1 Convolutions, etc., if applicable.]
  * **Loss Function:** [Briefly mention the loss function, typically negative log-likelihood for NFs.]

### Model Architecture Diagram

A visual representation of the CNF-UNet architecture helps in understanding the data flow and the interaction between the Normalizing Flow and UNet components.

*Conceptual diagram illustrating the integrated CNF-UNet architecture.*

[Remember to replace `model_architecture_diagram.png` with the actual path to your diagram image. You’ll need to create this diagram (e.g., using draw.io, Excalidraw, PowerPoint, or a scientific plotting library) and save it in `docs/source/_static/`.]

-----

## Evaluation Metrics

Evaluating the performance of generative models, especially for complex data like medical images, requires robust quantitative metrics in addition to qualitative visual inspection. This section outlines the key metrics employed to assess the quality, realism, and diversity of the synthetically generated Lung CT images from the CNF-UNet model, based on the `synthetic_image_generator.evaluate` module.

### Quantitative Metrics

We utilized a combination of established metrics to provide a comprehensive evaluation:

1.  **Mean Squared Error (MSE), Peak Signal-to-Noise Ratio (PSNR), and Structural Similarity Index (SSIM)**

      * **Purpose:** These metrics quantify the per-pixel and structural similarity between images.

          * **MSE** measures the average squared difference between pixels. A lower MSE indicates better pixel-wise similarity.
          * **PSNR** measures the ratio between the maximum possible pixel value and the power of distorting noise. A higher PSNR indicates better quality.
          * **SSIM** is a perceptual metric that quantifies image quality degradation based on structural information, luminance, and contrast changes. Values range from -1 to 1, with 1 being perfect similarity.

      * **Implementation in Project:** These metrics are calculated per-image by the `calculate_image_metrics` function within the `evaluate` module. They use `skimage.metrics` on NumPy arrays that are assumed to be normalized to the `[0, 1]` range. For `peak_signal_noise_ratio` and `structural_similarity`, a `data_range` of `1.0` is specified, and `channel_axis=None` for single-channel (grayscale) images. In the `evaluate_model` function, these metrics are computed for a `num_compare` subset of real and generated image pairs and then averaged.

      * **Results:** [Present your average MSE, PSNR, and SSIM scores here, along with interpretations. E.g., “Our model achieved an average MSE of X.XXX, PSNR of Y.YY dB, and SSIM of Z.ZZZ, suggesting a good fidelity at the pixel level on the compared samples.”]

2.  **Fréchet Inception Distance (FID)**

      * **Purpose:** FID is a more robust and perceptually relevant metric for assessing the quality and diversity of images generated by generative models. It measures the “distance” between the feature distributions of real and generated images, extracted from a pre-trained Inception-v3 network. A lower FID score indicates that the distribution of generated images is closer to the real image distribution, implying both higher quality and better diversity.
      * **Implementation in Project:** FID is calculated using the `torch_fidelity` library within the `evaluate_model` function. Before calculation, both real and generated images (which are initially in `[-1, 1]` pixel range) are processed using a `fid_transform` (which typically denormalizes them to `[0, 255]` and converts them to PIL Image format). These preprocessed images are then temporarily saved to disk in designated `fid_real_images` and `fid_generated_images` directories. `torch_fidelity` then uses these directories as input to compute the FID score, leveraging CUDA if available. The temporary directories are cleaned up after calculation.
      * **Results:** [Present your FID scores here. E.g., “The FID score obtained was X.XX. This low score demonstrates that the generated images capture the overall statistical properties and diversity of the real dataset effectively.”]

### Qualitative Evaluation

Beyond quantitative metrics, visual inspection by human observers is crucial for generative models. This involves manually reviewing a diverse set of generated images to assess:

  * **Realism:** Do the images look like actual Lung CT scans?
  * **Anatomical Correctness:** Are anatomical structures (e.g., lungs, blood vessels) plausible?
  * **Diversity:** Does the model generate a wide variety of distinct images, or does it suffer from mode collapse?
  * **Artifacts:** Are there any noticeable artifacts, noise, or blurring?
  * **Clinical Relevance:** Do the images maintain clinical relevance, such as showing variations in lung conditions?

-----

## Testing Strategy

Robust testing is crucial for ensuring the reliability, correctness, and maintainability of the Synthetic Image Generator project. This section outlines the testing strategy employed and provides an overview of the unit test modules.


## Testing Framework and Approach

The project utilizes **`pytest`** as its primary testing framework due to its simplicity, extensibility, and rich set of features for writing clear and concise tests. **`unittest.mock`** is extensively used to isolate the units under test from their external dependencies (e.g., file system, network operations, PyTorch operations, other modules), ensuring that tests are fast, deterministic, and truly focused on the specific logic being verified.

The testing approach primarily focuses on **unit testing**, where individual functions, methods, or classes are tested in isolation. This allows for:

* **Early Bug Detection:** Identifying issues in small, isolated components before they propagate.
* **Code Quality Assurance:** Ensuring each piece of code behaves as expected and maintaining high standards.
* **Refactoring Confidence:** Providing a safety net when making changes to the codebase.
* **Documentation by Example:** Tests often serve as executable documentation for how code components are intended to be used.

### Code Coverage

To ensure robust test coverage, the project integrates `coverage.py`, configured via a **`.coveragerc`** file. This setup allows for the measurement of the percentage of source code executed by the tests, helping identify untested areas and promoting comprehensive testing. A high code coverage percentage indicates that a significant portion of the codebase is being exercised by the test suite, reducing the likelihood of undetected bugs.

### Running Tests

To run the full test suite and generate a code coverage report, execute the following command from the project's root directory:

```bash
pytest --cov=synthetic_image_generator --cov-report=term-missing --cov-report=html
````

  * `--cov=synthetic_image_generator`: Specifies the directory for which code coverage should be calculated.
  * `--cov-report=term-missing`: Displays a summary of coverage in the terminal, including lines not covered.
  * `--cov-report=html`: Generates a detailed HTML report (in the `htmlcov/` directory) for interactive exploration of covered and uncovered lines.

### Test Modules Overview

The project’s test suite is organized into several modules, each responsible for testing a specific part of the application’s functionality:

  * **`tests/test_main.py`**

      * **Purpose:** This module provides comprehensive unit testing of the main execution flow (`main.py`). It verifies the correct orchestration and interaction of various components, ensuring that functions are called with expected arguments and in the correct sequence.

  * **`tests/test_dataset.py`**

      * **Purpose:** This test suite covers the `dataset` module, including the `load_dicom_image` function and the `LungCTWithGaussianDataset` class. It ensures correct DICOM file loading, image preprocessing, and overall dataset behavior, including error handling for missing data.

  * **`tests/test_transforms.py`**

      * **Purpose:** This module contains unit tests for the `transforms` module, specifically `get_transforms` and `get_fid_transforms` functions. It verifies that image preprocessing and post-processing pipelines (e.g., resizing, type conversion, normalization/denormalization) behave as expected.

  * **`tests/test_model.py`**

      * **Purpose:** This module contains unit tests for the `CNF_UNet` architecture defined in the `model` module. It ensures the model's forward pass works correctly, verifies output tensor shapes and data types, and may include basic sanity checks for model initialization and parameter counts.

  * **`tests/test_load_model.py`**

      * **Purpose:** This module focuses on testing the `load_model_from_drive` function, which handles downloading pre-trained model weights and initializing the `CNF_UNet` model. It covers scenarios such as successful loading, handling CUDA unavailability (though less relevant for macOS MPS), file not found errors, and download failures.

  * **`tests/test_train.py`**

      * **Purpose:** This test suite verifies the `train_model` function within the `train` module. It ensures that the training loop correctly updates model weights, that the loss generally decreases over epochs, and that no NaN/Inf values are produced during training. Mocking is extensively used here to simulate data and model interactions without running a full training epoch.

  * **`tests/test_generate.py`**

      * **Purpose:** This module contains unit tests for the `generate_images` function in the `generate` module. It ensures that the image generation process produces outputs of the correct shape and type, and verifies that different initial noise inputs leads to distinct generated images. Mocking is used to isolate the generation logic from actual PyTorch device operations.

  * **`tests/test_evaluate.py`**

      * **Purpose:** This test suite covers the `evaluate` module, specifically the `evaluate_model` function. It ensures that image quality metrics (MSE, PSNR, SSIM, FID) are calculated correctly and that temporary directories required for FID calculation are properly managed and cleaned up. Mocking of external dependencies (e.g., FID calculation libraries, file system interactions) is crucial here.



-----

## Limitations

While the Conditional Normalizing Flow (CNF) based Synthetic Image Generator demonstrates promising capabilities in producing realistic Lung CT images, it's important to acknowledge certain limitations inherent to the model architecture, the nature of the data, and the project's scope. Understanding these limitations is crucial for interpreting the model’s performance and guiding future research.

1.  **Computational Intensity and Scalability**

      * **Explanation:** Normalizing Flows, by design, involve computing the determinant of the Jacobian matrix, which can be computationally intensive, especially for high-resolution images or deep flow architectures. Training these models requires significant computational resources (GPUs with large memory) and can be time-consuming. Generating a large number of high-resolution images can also be slow. This limits immediate scalability to extremely high-resolution (e.g., 512x512 or 1024x1024 and beyond) 3D medical volumes without substantial architectural or hardware advancements.

2.  **Fidelity vs. Diversity Trade-off and Subtle Artifacts**

      * **Explanation:** Achieving a perfect balance between generating highly realistic (high fidelity) images and ensuring a wide range of diverse samples (avoiding mode collapse) remains a challenge for all generative models, including CNFs. While quantitative metrics like FID aim to capture both, minor or subtle artifacts, imperceptible to current metrics, might still be present in generated images. These artifacts could be crucial in sensitive applications like medical diagnosis, requiring expert human review. The model might struggle with rare anatomical variations or pathological findings if they are not sufficiently represented in the training data.

3.  **Data Dependency and Generalization**

      * **Explanation:** Like all data-driven deep learning models, the performance and generalizability of the CNF model are highly dependent on the quality, size, and diversity of the training dataset. If the training data is biased, contains artifacts, or lacks representation of certain conditions or patient demographics, the generated images will reflect these limitations. Generating images for novel pathologies or unseen anatomical configurations, which were not present in the training set, remains a significant challenge.

4.  **Lack of Fine-Grained Controllability**

      * **Explanation:** While the conditional aspect of the CNF-UNet allows for generating images based on a latent variable (e.g., different types of noise leading to different images), achieving precise, fine-grained control over specific anatomical features or the explicit introduction/removal of particular pathologies is complex. Current conditioning mechanisms might offer coarse control, but manipulating very specific, localized characteristics (e.g., the exact size and location of a small nodule) is not straightforward.

5.  **Clinical Validation and Interpretability**

      * **Explanation:** The synthetic images produced by this research project are intended for research and development purposes (e.g., data augmentation, privacy-preserving sharing). They have not undergone rigorous clinical validation by medical professionals for diagnostic accuracy or suitability in real-world clinical workflows. Furthermore, the “black-box” nature of deep learning models means that understanding precisely *why* a certain image was generated or *how* a specific input led to a particular output remains challenging, which can be a barrier to trust and adoption in clinical settings.

These limitations highlight areas for future work and underscore that while synthetic data can be incredibly useful, its application, especially in critical domains like healthcare, requires careful consideration and further validation.

-----

## License

This project, Synthetic Image Generator, is made available under the terms of the [MIT License](https://achintha9533.github.io/Pattern-Recognition/license.html). This permissive license allows for wide use, modification, and distribution, while requiring attribution.

-----

## Contact

For inquiries, feedback, or collaborations related to the Synthetic Image Generator project, please use the contact information provided below.

### Project Author

  * **Name:** Kasun Achintha Perera
  * **Email:** [pereraachintha84@gmail.com](mailto:pereraachintha84@gmail.com)
  * **GitHub:** [https://github.com/Achintha9533](https://github.com/Achintha9533)
  * **LinkedIn:** [https://www.linkedin.com/in/kasun-achintha-perera-068a43174/](https://www.linkedin.com/in/kasun-achintha-perera-068a43174/)

### Project Repository

For technical issues, bug reports, feature requests, or contributions, please refer to the project’s GitHub repository:

  * **GitHub Repository:** [https://github.com/Achintha9533/Pattern-Recognition](https://github.com/Achintha9533/Pattern-Recognition)

We welcome your feedback and appreciate any contributions to improve this project.