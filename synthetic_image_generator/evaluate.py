# Synthetic Image Generator/evaluate.py

import torch
import numpy as np
from skimage.metrics import mean_squared_error, peak_signal_noise_ratio, structural_similarity
from torch_fidelity import calculate_metrics
from pathlib import Path
import shutil
from tqdm import tqdm
import logging
import torchvision.transforms as T # Imported for type hinting clarity
import os

# Configure logging for this module
logger = logging.getLogger(__name__)

"""
This module provides comprehensive functionalities for evaluating the performance
and quality of synthetic images generated by the Conditional Normalizing Flow (CNF)
model. It includes implementations for common image quality metrics such as
Mean Squared Error (MSE), Peak Signal-to-Noise Ratio (PSNR), Structural Similarity
Index (SSIM), and Fréchet Inception Distance (FID).

The evaluation process involves comparing generated images against real images from
the dataset to quantify aspects like fidelity (how realistic the images are)
and diversity (how varied the generated images are). Temporary directories are
managed to facilitate FID calculation, which requires saving images to disk.
"""

def calculate_image_metrics(
    real_image: np.ndarray,
    generated_image: np.ndarray
) -> Dict[str, float]:
    """
    Calculates Mean Squared Error (MSE), Peak Signal-to-Noise Ratio (PSNR),
    and Structural Similarity Index (SSIM) between a real and a generated image.

    This function quantifies the per-pixel and structural similarity between
    two images. MSE measures the average squared difference between pixels.
    PSNR measures the ratio between the maximum possible pixel value and
    the power of distorting noise that affects the quality of its representation.
    SSIM is a perceptual metric that quantifies image quality degradation
    (e.g., distortion, noise) based on structural information.

    Args:
        real_image (np.ndarray): The ground truth real image as a NumPy array.
                                 Expected to be 2D (H, W) or 3D (H, W, C).
                                 Pixel values are typically in [0, 1].
        generated_image (np.ndarray): The generated image as a NumPy array.
                                      Expected to be 2D (H, W) or 3D (H, W, C).
                                      Pixel values are typically in [0, 1].

    Returns:
        Dict[str, float]: A dictionary containing the calculated 'mse', 'psnr', and 'ssim' values.

    Potential Exceptions Raised:
        - ValueError: If image dimensions or data types are incompatible for metric calculation.
                      `skimage.metrics` functions may raise this internally.

    Example of Usage:
    ```python
    import numpy as np
    # Assume real_img_np and gen_img_np are preprocessed NumPy arrays in [0, 1] range
    real_img_np = np.random.rand(64, 64)
    gen_img_np = np.random.rand(64, 64)
    metrics = calculate_image_metrics(real_img_np, gen_img_np)
    print(f"MSE: {metrics['mse']:.4f}, PSNR: {metrics['psnr']:.4f}, SSIM: {metrics['ssim']:.4f}")
    ```

    Relationships with other functions:
    - This function is called by `evaluate_model` to compute per-image quality metrics.

    Explanation of the theory:
    - **Mean Squared Error (MSE):** Measures the average of the squares of the errors
      (the difference between estimated and true values). Lower MSE indicates better similarity.
      Formula: $MSE = (1/N) * \Sigma(real_i - gen_i)^2$
    - **Peak Signal-to-Noise Ratio (PSNR):** Expresses the ratio between the maximum
      possible power of a signal and the power of corrupting noise that affects the
      fidelity of its representation. Higher PSNR indicates better quality.
      Formula: $PSNR = 10 * log10(MAX_I^2 / MSE)$, where $MAX_I$ is the maximum pixel value.
    - **Structural Similarity Index (SSIM):** A perceptual metric that considers
      image degradation as a perceived change in structural information, also incorporating
      luminance and contrast changes. Values range from -1 to 1, where 1 means perfect similarity.

    References for the theory:
    - Wang, Z., Bovik, A. C., Sheikh, H. R., & Simoncelli, E. P. (2004). Image quality
      assessment: from error visibility to structural similarity. IEEE transactions on
      image processing, 13(4), 600-612.
    """
    metrics: Dict[str, float] = {}
    try:
        # Ensure images are float and within [0, 1] if not already (SSIM expects this)
        real_image = real_image.astype(np.float64) # SSIM often expects float64
        generated_image = generated_image.astype(np.float64)

        metrics['mse'] = mean_squared_error(real_image, generated_image)
        # PSNR requires data_range (max pixel value - min pixel value).
        # Assuming images are normalized to [0, 1] after denormalization.
        metrics['psnr'] = peak_signal_noise_ratio(real_image, generated_image, data_range=1.0)
        # SSIM can take data_range. For single channel, channel_axis=None.
        metrics['ssim'] = structural_similarity(real_image, generated_image, data_range=1.0, channel_axis=None)
    except Exception as e:
        logger.error(f"Error calculating image metrics: {e}")
        # Return default values in case of error
        metrics['mse'] = float('inf')
        metrics['psnr'] = 0.0
        metrics['ssim'] = -1.0
    return metrics

def evaluate_model(
    real_images_batch_tensor: torch.Tensor,
    generated_images: torch.Tensor,
    fid_transform: T.Compose,
    num_compare: int
) -> None:
    """
    Calculates and logs various image quality metrics (MSE, PSNR, SSIM, and FID)
    to assess the fidelity and diversity of generated images compared to real ones.

    This function orchestrates the evaluation process. It takes batches of real
    and generated images, computes pixel-based metrics (MSE, PSNR, SSIM) on a
    subset, and then calculates the Fréchet Inception Distance (FID) for a larger
    set of images. FID is a more robust metric that captures both image quality
    and diversity by comparing feature representations from a pre-trained Inception model.
    Temporary directories are created to store images for FID calculation and are
    cleaned up afterwards.

    Args:
        real_images_batch_tensor (torch.Tensor): A batch of real images (CPU tensor)
                                                 to be used as ground truth.
                                                 Expected pixel range: [-1, 1].
        generated_images (torch.Tensor): A batch of generated images (CPU tensor)
                                         from the model. Expected pixel range: [-1, 1].
        fid_transform (T.Compose): A torchvision.transforms.Compose object used to
                                   prepare images for FID calculation (e.g., denormalize
                                   to [0, 255] and convert to PIL Image).
        num_compare (int): The number of image pairs (real vs. generated) to use
                           for calculating per-pixel metrics (MSE, PSNR, SSIM).
                           FID uses all available generated images.

    Returns:
        None: This function primarily performs side effects (logging metrics).

    Potential Exceptions Raised:
        - FileNotFoundError, OSError: During temporary directory creation or cleanup.
        - Exception from `torch_fidelity.calculate_metrics`: If FID calculation fails due
          to issues with the library, image formats, or Inception model download.

    Example of Usage:
    ```python
    import torch
    import torchvision.transforms as T
    from .transforms import get_fid_transforms # Assuming this is available
    # Assuming real_images and generated_images are torch.Tensors in [-1, 1] range
    # real_images = torch.randn(10, 1, 64, 64) * 0.5 + 0.5 # Example real images
    # generated_images = torch.randn(10, 1, 64, 64) * 0.5 + 0.5 # Example generated images
    # fid_trans = get_fid_transforms()
    # evaluate_model(real_images, generated_images, fid_trans, num_compare=5)
    ```

    Relationships with other functions:
    - Calls `calculate_image_metrics` for MSE, PSNR, SSIM.
    - Utilizes `torch_fidelity.calculate_metrics` for FID.
    - Relies on `torchvision.transforms` for image preparation for FID.
    - Called from `main.py` to perform overall model evaluation.

    Explanation of the theory:
    - **Fréchet Inception Distance (FID):** A metric used to assess the quality of
      images generated by generative models. It measures the distance between the
      feature vectors of real and generated images, extracted from a pre-trained
      Inception v3 network. A lower FID score indicates better quality and diversity
      of generated images, as it implies that the generated image distribution
      is closer to the real image distribution. FID is generally preferred over
      pixel-wise metrics (like MSE/PSNR) because it correlates better with human
      perceptual judgment of image quality.

    References for the theory:
    - Heusel, M., Ramsauer, H., Unterthiner, T., Nessler, B., & Hochreiter, S. (2017).
      Gans trained by a two time-scale update rule converge to a nash equilibrium.
      In Advances in neural information processing systems (pp. 6626-6637).
    """
    logger.info("Starting model evaluation...")

    # --- Calculate per-pixel metrics (MSE, PSNR, SSIM) for a small subset ---
    # These metrics are sensitive to exact pixel alignment and may not fully
    # capture perceptual quality or diversity, but provide a basic sanity check.
    mse_values = []
    psnr_values = []
    ssim_values = []

    # Ensure tensors are on CPU and convert to numpy for skimage
    real_cpu_np = real_images_batch_tensor[:num_compare, 0].cpu().numpy()
    gen_cpu_np = generated_images[:num_compare, 0].cpu().numpy()

    for i in range(min(num_compare, real_cpu_np.shape[0], gen_cpu_np.shape[0])):
        metrics = calculate_image_metrics(real_cpu_np[i], gen_cpu_np[i])
        mse_values.append(metrics['mse'])
        psnr_values.append(metrics['psnr'])
        ssim_values.append(metrics['ssim'])

    logger.info(f"Average MSE: {np.mean(mse_values):.6f}")
    logger.info(f"Average PSNR: {np.mean(psnr_values):.4f}")
    logger.info(f"Average SSIM: {np.mean(ssim_values):.4f}")

    # --- Calculate Fréchet Inception Distance (FID) ---
    # FID requires saving images to a temporary directory.
    # It compares the feature distributions of real vs. generated images.
    real_images_dir = Path("fid_real_images")
    generated_images_dir = Path("fid_generated_images")

    # Clean up any existing directories from previous runs
    shutil.rmtree(real_images_dir, ignore_errors=True)
    shutil.rmtree(generated_images_dir, ignore_errors=True)

    real_images_dir.mkdir(parents=True, exist_ok=True)
    generated_images_dir.mkdir(parents=True, exist_ok=True)

    try:
        logger.info("Saving images for FID calculation...")
        # Save real images
        for i, img_tensor in enumerate(tqdm(real_images_batch_tensor, desc="Saving Real Images for FID")):
            # Apply FID transform (de-normalize to [0,1] and convert to PIL, then save)
            img_pil = fid_transform(img_tensor)
            img_pil.save(real_images_dir / f"real_{i:04d}.png")

        # Save generated images
        for i, img_tensor in enumerate(tqdm(generated_images, desc="Saving Generated Images for FID")):
            # Apply FID transform
            img_pil = fid_transform(img_tensor)
            img_pil.save(generated_images_dir / f"generated_{i:04d}.png")

        logger.info("Calculating FID...")
        metrics = calculate_metrics(
            input1=str(real_images_dir),
            input2=str(generated_images_dir),
            cuda=torch.cuda.is_available(), # Use CUDA if available
            metrics=['fid'],
            # This ensures that the InceptionV3 model used by torch_fidelity can be downloaded.
            # Usually not needed if `~/.cache/torch/hub` is writable.
            # Also, set a low `num_workers` to avoid multiprocessing issues if any.
            num_workers=0 if os.name == 'nt' else 1, # Set to 0 for Windows, 1 for other OS
            verbose=False # Suppress verbose output from torch_fidelity
        )

        if 'fid' in metrics:
            logger.info(f"FID: {metrics['fid']:.4f}")
        elif 'frechet_inception_distance' in metrics: # Handle potential alternative key
            logger.info(f"FID: {metrics['frechet_inception_distance']:.4f}")
        else:
            logger.error("FID value not found in metrics. This might indicate an internal error in torch_fidelity.")
            logger.debug(f"Full metrics dictionary: {metrics}")

    except Exception as e:
        # Log detailed error information if FID calculation fails.
        logger.error(f"An error occurred during FID calculation: {e}")
        logger.error("Please ensure 'torch_fidelity' is installed (`pip install torch_fidelity`), "
                     "that you have the necessary torchvision/Pillow dependencies for image saving, "
                     "and that Inception V3 model weights can be downloaded (torch_fidelity handles this usually).")
    finally:
        # Always attempt to clean up temporary directories, regardless of success or failure.
        try:
            if real_images_dir.exists():
                shutil.rmtree(real_images_dir)
            if generated_images_dir.exists():
                shutil.rmtree(generated_images_dir)
            logger.info("Temporary FID image directories cleaned up.")
        except OSError as e:
            logger.error(f"Error during final cleanup of temporary FID directories: {e}. Manual deletion may be required.")